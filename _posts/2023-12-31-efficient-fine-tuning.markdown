---
layout: post
title:  "Review of Efficient Finetuning Techniques"
date:   2023-01-01
categories: ["deeplearning", "finetuning"]
author: "HK"
---

## Gradientless Methods
1. Jin, F., Zhang, J. and Zong, C., 2023, December. [Parameter-efficient Tuning for Large Language Model without Calculating Its Gradients](https://aclanthology.org/2023.emnlp-main.22/){:target="_blank"}. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (pp. 321-330). [[Review]]({% post_url 2023-12-31-gradientless-emnlp2023 %}){:target="_blank"}

# Others:
1. Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M. and Gelly, S., 2019, May. [Parameter-efficient transfer learning for NLP](https://proceedings.mlr.press/v97/houlsby19a.html){:target="_blank"}. In International Conference on Machine Learning (pp. 2790-2799). PMLR.
2. Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L. and Chen, W., 2021. [Lora: Low-rank adaptation of large language models](https://arxiv.org/pdf/2106.09685.pdf){:target="_blank"}. In The Tenth International Conference on Learning Representation. ICLR.
3. Li XL, Liang P. [Prefix-tuning: Optimizing continuous prompts for generation](https://arxiv.org/pdf/2101.00190.pdf){:target="_blank"}. In The 11th International Joint Conference on Natural Language Processing. ACL-IJCNLP.
